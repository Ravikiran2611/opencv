{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer(cuda).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikiran2611/opencv/blob/master/transformer(cuda).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj3C5F9eP4ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rzmQSiKdELi",
        "colab_type": "code",
        "outputId": "c2b42282-3dfa-4ba7-957a-4d67e73969b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "!python -m spacy download de"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz#egg=de_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh9hICIk7cXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datacreating\n",
        "from torchtext import data, datasets\n",
        "if True:\n",
        "    import spacy\n",
        "    spacy_de = spacy.load('de')\n",
        "    spacy_en = spacy.load('en')\n",
        "\n",
        "    def tokenize_de(text):\n",
        "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "    BOS_WORD = '<s>'\n",
        "    EOS_WORD = '</s>'\n",
        "    BLANK_WORD = \"<blank>\"\n",
        "    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
        "    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                     eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "    MAX_LEN = 100\n",
        "    train, val, test = datasets.IWSLT.splits(\n",
        "        exts=('.en', '.de'), fields=(SRC, TGT), \n",
        "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "            len(vars(x)['trg']) <= MAX_LEN)\n",
        "    MIN_FREQ = 2\n",
        "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ip5Gkl9fudf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator,val_iterator,text_iterator = data.BucketIterator.splits((train, val, test), batch_size = 64,sort_within_batch = True, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhizMKkYQHFI",
        "colab_type": "code",
        "outputId": "6c34164d-b04a-45e5-c6bb-7a7a1789e998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_iterator)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akQsE8kAHP1V",
        "colab_type": "code",
        "outputId": "cf6d950c-6fd4-4aa3-91a7-830c9b5bfd3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_iterator"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.iterator.BucketIterator at 0x7fec21006940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXgFuJ-YQI4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VInUFPrQ_yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from torch.autograd import Variable\n",
        "class PositionalEncoder(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "                \n",
        "#         since in transformer there is no recurrence involved , it does know about the order of the words in the sentence.\n",
        "#         For that purpose we add postional encoding that contains inforation about the relative or absolute postions of the words in the sentence.\n",
        "#         the size of postional encoding is equal to the word embedding size\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0.0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHPyDgm-R3z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)]) \n",
        "# provides multiple copies of the given modules"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avs4wCrzRNrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "#         core part of transformer and this function is just multiple time  attention function\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "              \n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        \n",
        "       \n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        \n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
        "        output = self.out(concat)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb8QaAdmRUJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "#   in attention function it tries to calculate , how important the other words are to specific words in the sentence\n",
        "#  so at a single time step  each word will be able to see all the other words in the sentence \n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    scores = torch.nn.functional.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhiUxbiaRZns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.nn.functional.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q7g05uURqA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmUgVCusRujH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "#   the encoder layer consist of two functions, one is multihead attention anf another is feed forward layer \n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHkOwdB08bxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "#   the decoder layer consist of three functions ,one is masked multi head attention which makes sures that the word in the \n",
        "# target sentence is only based on the previous words and this is done by masking the future words in te target sentence.\n",
        "# The second is a normal multihead attention layer followed by the feed forward layer\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "#         print(x.shape)\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
        "        src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0wUOSaMR0Jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "#   consist of  N number of encoder layer (here the value of  N is 6)\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfS1HzLh8EfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "#   consist of  N number of decoder layer (here the value of  N is 6)\n",
        "  \n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2mGYdJ68GBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "#   the transformer consist of encoder followed by the decoder .The decoder inputs is passed to the last linear layer \n",
        "# whose size is equal to the vocab size of target language. Softmax is applied by the loss function (cross entropy)\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        print(\"encoder over\")\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        print(\"decoder over\")\n",
        "        output = self.out(d_output)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgUx6TaASqJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_model = 512\n",
        "heads = 8\n",
        "N = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agYXZ0E78OPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_vocab = len(SRC.vocab)\n",
        "trg_vocab = len(TGT.vocab)\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)\n",
        "model = model.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUJEs8q4YFcA",
        "colab_type": "code",
        "outputId": "cef787f1-4a88-490c-d8ba-c1641aa947a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "for i, batch in enumerate(train_iterator):\n",
        "  src = batch.src.transpose(0,1)\n",
        "  trg = batch.trg.transpose(0,1)\n",
        "          \n",
        "  trg_input = trg[:, :-1]\n",
        "          \n",
        "  optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "  targets = trg[:, 1:].contiguous().view(-1)\n",
        "        \n",
        "\n",
        "            \n",
        "  input_pad = SRC.vocab.stoi['<blank>']\n",
        "  input_msk = (src != input_pad).unsqueeze(1)\n",
        "\n",
        "            \n",
        "  target_pad = TGT.vocab.stoi['<blank>']\n",
        "  target_msk = (trg_input != target_pad).unsqueeze(1)\n",
        "  size = trg_input.size(1)\n",
        "  shape = (1,size,size)\n",
        "  nopeak_mask = np.triu(np.ones(shape),k=1).astype('uint8')\n",
        "  nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
        "  target_msk = target_msk & nopeak_mask.cuda()\n",
        "\n",
        " \n",
        "  preds = model(src, trg_input, input_msk, target_msk)\n",
        "  optim.zero_grad()\n",
        "  loss = torch.nn.functional.cross_entropy(preds.view(-1, preds.size(-1)),\n",
        "  targets, ignore_index=target_pad)\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  print(i)\n",
        "  if(i == 2000):\n",
        "       break"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder over\n",
            "decoder over\n",
            "0\n",
            "encoder over\n",
            "decoder over\n",
            "1\n",
            "encoder over\n",
            "decoder over\n",
            "2\n",
            "encoder over\n",
            "decoder over\n",
            "3\n",
            "encoder over\n",
            "decoder over\n",
            "4\n",
            "encoder over\n",
            "decoder over\n",
            "5\n",
            "encoder over\n",
            "decoder over\n",
            "6\n",
            "encoder over\n",
            "decoder over\n",
            "7\n",
            "encoder over\n",
            "decoder over\n",
            "8\n",
            "encoder over\n",
            "decoder over\n",
            "9\n",
            "encoder over\n",
            "decoder over\n",
            "10\n",
            "encoder over\n",
            "decoder over\n",
            "11\n",
            "encoder over\n",
            "decoder over\n",
            "12\n",
            "encoder over\n",
            "decoder over\n",
            "13\n",
            "encoder over\n",
            "decoder over\n",
            "14\n",
            "encoder over\n",
            "decoder over\n",
            "15\n",
            "encoder over\n",
            "decoder over\n",
            "16\n",
            "encoder over\n",
            "decoder over\n",
            "17\n",
            "encoder over\n",
            "decoder over\n",
            "18\n",
            "encoder over\n",
            "decoder over\n",
            "19\n",
            "encoder over\n",
            "decoder over\n",
            "20\n",
            "encoder over\n",
            "decoder over\n",
            "21\n",
            "encoder over\n",
            "decoder over\n",
            "22\n",
            "encoder over\n",
            "decoder over\n",
            "23\n",
            "encoder over\n",
            "decoder over\n",
            "24\n",
            "encoder over\n",
            "decoder over\n",
            "25\n",
            "encoder over\n",
            "decoder over\n",
            "26\n",
            "encoder over\n",
            "decoder over\n",
            "27\n",
            "encoder over\n",
            "decoder over\n",
            "28\n",
            "encoder over\n",
            "decoder over\n",
            "29\n",
            "encoder over\n",
            "decoder over\n",
            "30\n",
            "encoder over\n",
            "decoder over\n",
            "31\n",
            "encoder over\n",
            "decoder over\n",
            "32\n",
            "encoder over\n",
            "decoder over\n",
            "33\n",
            "encoder over\n",
            "decoder over\n",
            "34\n",
            "encoder over\n",
            "decoder over\n",
            "35\n",
            "encoder over\n",
            "decoder over\n",
            "36\n",
            "encoder over\n",
            "decoder over\n",
            "37\n",
            "encoder over\n",
            "decoder over\n",
            "38\n",
            "encoder over\n",
            "decoder over\n",
            "39\n",
            "encoder over\n",
            "decoder over\n",
            "40\n",
            "encoder over\n",
            "decoder over\n",
            "41\n",
            "encoder over\n",
            "decoder over\n",
            "42\n",
            "encoder over\n",
            "decoder over\n",
            "43\n",
            "encoder over\n",
            "decoder over\n",
            "44\n",
            "encoder over\n",
            "decoder over\n",
            "45\n",
            "encoder over\n",
            "decoder over\n",
            "46\n",
            "encoder over\n",
            "decoder over\n",
            "47\n",
            "encoder over\n",
            "decoder over\n",
            "48\n",
            "encoder over\n",
            "decoder over\n",
            "49\n",
            "encoder over\n",
            "decoder over\n",
            "50\n",
            "encoder over\n",
            "decoder over\n",
            "51\n",
            "encoder over\n",
            "decoder over\n",
            "52\n",
            "encoder over\n",
            "decoder over\n",
            "53\n",
            "encoder over\n",
            "decoder over\n",
            "54\n",
            "encoder over\n",
            "decoder over\n",
            "55\n",
            "encoder over\n",
            "decoder over\n",
            "56\n",
            "encoder over\n",
            "decoder over\n",
            "57\n",
            "encoder over\n",
            "decoder over\n",
            "58\n",
            "encoder over\n",
            "decoder over\n",
            "59\n",
            "encoder over\n",
            "decoder over\n",
            "60\n",
            "encoder over\n",
            "decoder over\n",
            "61\n",
            "encoder over\n",
            "decoder over\n",
            "62\n",
            "encoder over\n",
            "decoder over\n",
            "63\n",
            "encoder over\n",
            "decoder over\n",
            "64\n",
            "encoder over\n",
            "decoder over\n",
            "65\n",
            "encoder over\n",
            "decoder over\n",
            "66\n",
            "encoder over\n",
            "decoder over\n",
            "67\n",
            "encoder over\n",
            "decoder over\n",
            "68\n",
            "encoder over\n",
            "decoder over\n",
            "69\n",
            "encoder over\n",
            "decoder over\n",
            "70\n",
            "encoder over\n",
            "decoder over\n",
            "71\n",
            "encoder over\n",
            "decoder over\n",
            "72\n",
            "encoder over\n",
            "decoder over\n",
            "73\n",
            "encoder over\n",
            "decoder over\n",
            "74\n",
            "encoder over\n",
            "decoder over\n",
            "75\n",
            "encoder over\n",
            "decoder over\n",
            "76\n",
            "encoder over\n",
            "decoder over\n",
            "77\n",
            "encoder over\n",
            "decoder over\n",
            "78\n",
            "encoder over\n",
            "decoder over\n",
            "79\n",
            "encoder over\n",
            "decoder over\n",
            "80\n",
            "encoder over\n",
            "decoder over\n",
            "81\n",
            "encoder over\n",
            "decoder over\n",
            "82\n",
            "encoder over\n",
            "decoder over\n",
            "83\n",
            "encoder over\n",
            "decoder over\n",
            "84\n",
            "encoder over\n",
            "decoder over\n",
            "85\n",
            "encoder over\n",
            "decoder over\n",
            "86\n",
            "encoder over\n",
            "decoder over\n",
            "87\n",
            "encoder over\n",
            "decoder over\n",
            "88\n",
            "encoder over\n",
            "decoder over\n",
            "89\n",
            "encoder over\n",
            "decoder over\n",
            "90\n",
            "encoder over\n",
            "decoder over\n",
            "91\n",
            "encoder over\n",
            "decoder over\n",
            "92\n",
            "encoder over\n",
            "decoder over\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ed29e76c371f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   loss = torch.nn.functional.cross_entropy(preds.view(-1, preds.size(-1)),\n\u001b[1;32m     30\u001b[0m   targets, ignore_index=target_pad)\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.42 GiB (GPU 0; 11.17 GiB total capacity; 10.21 GiB already allocated; 112.56 MiB free; 534.37 MiB cached)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeVQ11JkihPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def translate(model, src, max_len = 10, custom_string=False):\n",
        "    device1 =  torch.device('cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device1)\n",
        "    input_pad = SRC.vocab.stoi['<blank>']\n",
        "    if custom_string == True:\n",
        "        src = tokenize_en(src)\n",
        "        sentence=Variable(torch.LongTensor([[SRC.vocab.stoi[tok] for tok in src]]))\n",
        "    src_mask = (sentence != input_pad).unsqueeze(-2)\n",
        "    e_outputs = model.encoder(sentence, src_mask)\n",
        "    outputs = torch.zeros(max_len).type_as(sentence.data)\n",
        "    outputs[0] = torch.LongTensor([TGT.vocab.stoi['<sos>']])\n",
        "    for i in range(1, max_len):    \n",
        "        shape = (1,i,i)    \n",
        "        testtrg_mask = np.triu(np.ones(shape), k=1).astype('uint8')\n",
        "        \n",
        "        testtrg_mask = Variable(torch.from_numpy(testtrg_mask) == 0)\n",
        "        out = model.decoder(outputs[:i].unsqueeze(0), e_outputs, src_mask, testtrg_mask)\n",
        "        out = model.out(out)\n",
        "        out = torch.nn.functional.softmax(out, dim=-1)\n",
        "        val, ix = out[:, -1].data.topk(1) # returns the max value and the position of the max value\n",
        "        outputs[i] = ix[0][0]\n",
        "        if ix[0][0] == TGT.vocab.stoi['<eos>']:\n",
        "            break\n",
        "    return ' '.join([TGT.vocab.itos[ix] for ix in outputs[:i]] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64wDt7TdidmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translated_sentence = translate(model,'have a nice day',40,custom_string= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94WY6TOVif0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translated_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}